---
permalink: /research/
title: "My Researchs"
author_profile: true
---

### [VITCAE: ViT-based Class-conditioned Autoencoder](https://arxiv.org/abs/2509.16554)
![VITCAE Architecture](/images/vitcae_architecture.png)
Standard Vision Transformer (ViT) autoencoders often fail to leverage the global Class token for generation and use static attention mechanisms, which is inefficient. VITCAE addresses this by transforming the Class token into a generative controller. In this framework, the encoder maps the Class token to a global latent variable that sets the prior distribution for all local patch-level latents, creating a strong hierarchy where global semantics guide the synthesis of local details. Inspired by opinion dynamics, we introduce a convergence-aware temperature scheduler that adaptively anneals each attention head's influence, allowing us to freeze converged heads during training. This dynamic head-pruning mechanism significantly improves computational efficiency without sacrificing model fidelity.

### [Generative Expansion of Small Datasets: An Expansive Graph Approach](https://ieeexplore.ieee.org/document/10887596)
![Expansive Synthesis Architecture](/images/expansive_synthesis.png)
Data scarcity is a major bottleneck in machine learning. This work introduces an "Expansive Synthesis" model designed to generate large-scale, information-rich datasets from minimal initial samples. The framework leverages the non-linear latent space of a neural network, interpreted through Koopman operator theory, to create a linearized space suitable for principled expansion. Using expander graph mappings and feature interpolation, the model generates new, diverse data points while preserving the statistical distribution and feature relationships of the original data. The distributional consistency of the newly synthesized data is refined using an autoencoder with self-attention and regularized by an optimal transport loss.

### [Koopcon: A new approach towards smarter and less complex learning](https://ieeexplore.ieee.org/document/10647948)
![Koopcon Architecture](/images/koopcon_architecture.png)
Training deep learning models on massive datasets is computationally expensive. Koopcon introduces an autoencoder-based dataset condensation model that packs large datasets into compact, information-rich representations. The model is backed by Koopman operator theory, which allows us to systematically handle the non-linear features of the data in a linear latent space. To ensure the condensed dataset faithfully represents the original, the model minimizes the distributional discrepancy using Optimal Transport theory and the Wasserstein distance. Experimental results show that classifiers trained on the small, condensed dataset generated by Koopcon exhibit performance comparable to those trained on the original large dataset, affirming the model's efficacy in creating data-efficient learning pipelines.
